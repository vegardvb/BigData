{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83045409-70a5-4010-b1cf-9db8d98f2bad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059a269d-8f13-494e-84e5-9da5d873047b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Name of person A Vegard Vaeng Bernhardsen\n",
    "\n",
    "Name of person B None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52939e3d-35c7-4767-a59e-ba75f716e952",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-ac06c9a4-eac3-44e1-940e-3bf0904510fd/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499e46f5-12a4-46de-a212-90f2e936461e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading modules that we need\n",
    "import unittest\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8c68e0-6bda-40a6-8588-381e51dc0a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n",
    "def load_df(table_name: \"name of the table to load\") -> DataFrame:\n",
    "    return spark.read.parquet(table_name)\n",
    "\n",
    "users_df = load_df(\"/user/hive/warehouse/users\")\n",
    "comments_df = load_df(\"/user/hive/warehouse/comments\")\n",
    "posts_df = load_df(\"/user/hive/warehouse/posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2274f0-1bd8-4812-83d2-f793587e9548",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: implenenting two helper functions\n",
    "Impelment these two functions:\n",
    "1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n",
    "2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n",
    "\n",
    "Note that the result of a Spark SQL query is itself a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fce811-7dd2-4602-a243-18a36754c302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n",
    "    # Get the current Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Register the DataFrame as a temporary view\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "    \n",
    "    # Execute the query\n",
    "    result_df = spark.sql(query)\n",
    "    \n",
    "    # Fetch the content of the first column of the first row\n",
    "    result = result_df.collect()[0][0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n",
    "   # Get the current Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Register both DataFrames as temporary views\n",
    "    df1.createOrReplaceTempView(\"tempView1\")\n",
    "    df2.createOrReplaceTempView(\"tempView2\")\n",
    "    \n",
    "    # Execute the query\n",
    "    result_df = spark.sql(query)\n",
    "    \n",
    "    # Fetch the content of the first column of the first row\n",
    "    result = result_df.collect()[0][0]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a021cce-880b-42b1-a09d-6d2c2222124e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n"
     ]
    }
   ],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6108c65-e86a-47f3-8c8d-ef64b9c0b78b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "posts_df.createOrReplaceTempView(\"posts\")\n",
    "users_df.createOrReplaceTempView(\"users\")\n",
    "comments_df.createOrReplaceTempView(\"comments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de10aba-7e77-4baa-af35-2b4e37916071",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: writing a few queries\n",
    "Write the following queries in SQL to be executed by Spark in the next cell.\n",
    "\n",
    "1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n",
    "2. 'q2': find the number users\n",
    "3. 'q3': find the 'Id' of the user who posted most number of answers\n",
    "4. 'q4': find the number of questions\n",
    "5. 'q5': find the display name of the user who posted most number of comments\n",
    "\n",
    "Note that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3273ef7-7f4d-4b5d-bcf7-9935c952e26d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q1 = \"SELECT * FROM posts ORDER BY CreationDate DESC limit 1\"\n",
    "\n",
    "q2 = \"SELECT COUNT(*) AS NumberOfUsers FROM users\"\n",
    "\n",
    "q3 = \"SELECT OwnerUserId AS UserId, COUNT(*) AS NumberOfAnswers FROM posts WHERE PostTypeId = 2 GROUP BY OwnerUserId ORDER BY NumberOfAnswers DESC LIMIT 1\"\n",
    "\n",
    "q4 = \"SELECT COUNT(*) AS NumberOfQuestions FROM posts WHERE PostTypeId = 1\"\n",
    "\n",
    "# For q5, assuming you need to join users and comments, adjust the query to fit your data structure and temporary view names\n",
    "q5 = \"SELECT DisplayName FROM users JOIN comments ON users.Id = comments.UserId GROUP BY DisplayName ORDER BY COUNT(*) DESC LIMIT 1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46400bac-c11c-4bee-81a8-67d42e3ca968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementations by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ca3fb9-427c-425d-b334-0df9c208a21b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".....\n----------------------------------------------------------------------\nRan 5 tests in 7.775s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n----------------------------------------------------------------------\nRan 5 tests in 7.775s\n\nOK\nOut[118]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"
     ]
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask2(unittest.TestCase):\n",
    "    def test_q1(self):\n",
    "        # find the id of the most recent post\n",
    "        r = run_query(q1, posts_df)\n",
    "        self.assertEqual(r, 95045)\n",
    "\n",
    "    def test_q2(self):\n",
    "        # find the number of the users\n",
    "        r = run_query(q2, users_df)\n",
    "        self.assertEqual(r, 91616)\n",
    "\n",
    "    def test_q3(self):\n",
    "        # find the user id of the user who posted most number of answers\n",
    "        r = run_query(q3, posts_df)\n",
    "        self.assertEqual(r, 64377)\n",
    "\n",
    "    def test_q4(self):\n",
    "        # find the number of questions\n",
    "        r = run_query(q4, posts_df)\n",
    "        self.assertEqual(r, 28950)\n",
    "\n",
    "    def test_q5(self):\n",
    "        # find the display name of the user who posted most number of comments\n",
    "        r = run_query2(q5, users_df, comments_df)\n",
    "        self.assertEqual(r, \"Neil Slater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8a2e0c-7e72-410b-b385-00503c0bc136",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n",
    "2. When do you suggest using RDDs instead of using DataFrames?\n",
    "3. What is the main benefit of using DataSets instead of DataFrames?\n",
    "\n",
    "Write your answers in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c33ca21-1672-4c0c-a876-f73cbb8f65b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Your answers...\n",
    "\n",
    "1:\n",
    "RDD is the foundational data structure in Spark. it represents an immutable, distributed collection of objects that can be processed in parallel. For processing tasks, RDDs provide low level functionality and fine grained control over data processing tasks, which allows users to apply custom transformations and actions. They are resilient against failures, as they leverage lineage information to compute lost data. \n",
    "\n",
    "Dataframe is a distributed collection of data, which is organized into named columns. Which is similar to a table in a relational database. DataFrames are in fact built on top of RDDs and provide a higher level abstrction, which allows users to use SQL like operations to manipulate data. By using Spark's Catalyst optimizer to optimize, it can lead to more efficient execution plans. \n",
    "\n",
    "Datasets are a type-safe version of DataFrames which provides the benefits of RDDs with the optimizations of DataFrames. Datasets are available in statically typed languages as Java and Scala. This allows for compile-time type checking and more efficient de/serialization. They offer a blend of RDD's functional programming capabilities and DataFrame's optimization stratergies. \n",
    "\n",
    "2:\n",
    "RDDs are better suited for low level transformations and actions requiring fine-grained control over the data processing tasks. Especially when you're working with unstructured data (text files) or while performing comples operations that are not easy to express in SQL. Furthermore, when you need to maintain precise control over physical data distribution and partitioning across clusters to optimize the performance for some types of computations.\n",
    "\n",
    "3:\n",
    "The main benefit for using Datasets over DataFrames is the type safety provided. As DataFrames offer a dynamic schema and allow for easy and epressive manipulations of structured data, they do not however provide compile-time type checking. Which might lead to runtime errors if the ata is accessed using the incorrect column names or data types. \n",
    "\n",
    "Datasets however, offer \"the ebst of both worlds\". They provide the same level of expressiveness as DataFrames, while still providing type safety. Which allows for errors to be caught at compile time in statically typed languages. Making Datasets useful for applications requiring the robustness of type checking together with the performance optimizations of Spark SQL's engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a90247-62d6-4d15-b0a1-49eb936cca49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1872579956312257,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Task2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
