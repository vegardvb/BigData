{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d8965a-c0b4-4802-abb8-05fc5e21ce59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c623c5-86c0-48ee-85de-5d514b4b334f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Name of person A: Vegard Vaeng Bernhardsen\n",
    "\n",
    "Name of person B: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "324eb902-51f2-44b0-8ff1-730efac9900c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import dbutils\n",
    "# # Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\n",
    "# dbutils.fs.rm(\"/user\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b530d6-b580-4de2-affb-aad878f4da86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023f4d10-e729-450c-8d1a-50d494b488d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading PySpark modules that we need\n",
    "import unittest\n",
    "from collections import Counter\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession  # Make sure to import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcda919c-b51e-4b61-9d61-80cc24f2d15e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: defining the schema for the data\n",
    "Typically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8bdba5-f6d6-43bf-bed9-763d99cfcc91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining a schema for 'badges' table\n",
    "badges_schema = StructType([StructField('UserId', IntegerType(), False),\n",
    "                            StructField('Name', StringType(), False),\n",
    "                            StructField('Date', TimestampType(), False),\n",
    "                            StructField('Class', IntegerType(), False)])\n",
    "\n",
    "# Defining a schema for 'posts' table\n",
    "posts_schema = StructType([\n",
    "    StructField('Id', IntegerType(), False),\n",
    "    StructField('ParentId', IntegerType(), True),  # Nullable because only answers have a ParentId\n",
    "    StructField('PostTypeId', IntegerType(), False),\n",
    "    StructField('CreationDate', TimestampType(), False),\n",
    "    StructField('Score', IntegerType(), False),\n",
    "    StructField('ViewCount', IntegerType(), True),  # Nullable because it might not be relevant for answers\n",
    "    StructField('Body', StringType(), False), # Note: This will be base64 encoded\n",
    "    StructField('OwnerUserId', IntegerType(), True),  # Nullable because a post might not have an owner (rare cases)\n",
    "    StructField('LastActivityDate', TimestampType(), False),\n",
    "    StructField('Title', StringType(), True),  # Nullable because only questions have a title\n",
    "    StructField('Tags', StringType(), True),  # Nullable because only questions have tags\n",
    "    StructField('AnswerCount', IntegerType(), True),  # Nullable because only questions have answers\n",
    "    StructField('CommentCount', IntegerType(), False),\n",
    "    StructField('FavoriteCount', IntegerType(), True),  # Nullable because only questions might be favorited\n",
    "    StructField('CloseDate', TimestampType(), True)  # Nullable because only questions might be closed\n",
    "])\n",
    "\n",
    "# Defining a schema for 'users' table\n",
    "users_schema = StructType([\n",
    "    StructField('Id', IntegerType(), False),\n",
    "    StructField('Reputation', IntegerType(), False),\n",
    "    StructField('CreationDate', TimestampType(), False),\n",
    "    StructField('DisplayName', StringType(), False),\n",
    "    StructField('LastAccessDate', TimestampType(), False),\n",
    "    StructField('AboutMe', StringType(), True),  # Nullable because a user might not have filled this out\n",
    "    StructField('Views', IntegerType(), False),\n",
    "    StructField('UpVotes', IntegerType(), False),\n",
    "    StructField('DownVotes', IntegerType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "# Defining a schema for 'comments' table\n",
    "comments_schema = StructType([\n",
    "    StructField('PostId', IntegerType(), False),\n",
    "    StructField('Score', IntegerType(), False),\n",
    "    StructField('Text', StringType(), False),  # Note: This will be base64 encoded\n",
    "    StructField('CreationDate', TimestampType(), False),\n",
    "    StructField('UserId', IntegerType(), True)  # Nullable because a comment might not be linked to a user (anonymous comments)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: implementing two helper functions\n",
    "Next, we need to implement two helper functions:\n",
    "1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n",
    "2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n",
    "\n",
    "Note that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n",
    "\n",
    "BTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089f87ff-f2c8-4ac8-8449-cec251c502f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_csv(source_file: str, schema: StructType) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a Spark DataFrame using the provided schema.\n",
    "    \n",
    "    :param source_file: Path for the CSV file to load.\n",
    "    :param schema: Schema for the CSV file being loaded as a DataFrame.\n",
    "    :return: Spark DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    # Ensure SparkSession is imported and available\n",
    "    spark = SparkSession.builder.appName(\"Big Data Application\").getOrCreate()\n",
    "    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n",
    "    return df\n",
    "\n",
    "def save_df(df: DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves a Spark DataFrame as a Parquet file on DBFS.\n",
    "    \n",
    "    :param df: DataFrame to be saved.\n",
    "    :param table_name: Name under which the DataFrame will be saved. This should include the path.\n",
    "    \"\"\"\n",
    "    df.write.mode('overwrite').parquet(f\"/user/hive/warehouse/{table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39bc683c-b37a-4842-8bf8-004620b17cca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8903e903-7e4f-4c15-99bd-c9129a601fde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementation by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n",
    "\n",
    "Note that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd470d59-2571-4b7d-b022-9ee8f7c3e281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "salmon",
       "message": "EEEEE\n======================================================================\nERROR: test_load_badges (__main__.TestTask1.test_load_badges)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 5, in test_load_badges\n  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/badges_csv.gz.\n\n======================================================================\nERROR: test_load_comments (__main__.TestTask1.test_load_comments)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 30, in test_load_comments\n  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/comments_csv.gz.\n\n======================================================================\nERROR: test_load_posts (__main__.TestTask1.test_load_posts)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 16, in test_load_posts\n  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/posts_csv.gz.\n\n======================================================================\nERROR: test_load_users (__main__.TestTask1.test_load_users)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 41, in test_load_users\n  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/users_csv.gz.\n\n======================================================================\nERROR: test_save_dfs (__main__.TestTask1.test_save_dfs)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 60, in test_save_dfs\n  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/users_csv.gz.\n\n----------------------------------------------------------------------\nRan 5 tests in 6.704s\n\nFAILED (errors=5)\n",
       "previous": 0
      },
      "text/plain": [
       "Fail"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEEEE\n",
      "======================================================================\n",
      "ERROR: test_load_badges (__main__.TestTask1.test_load_badges)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 5, in test_load_badges\n",
      "  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n",
      "    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/badges_csv.gz.\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_load_comments (__main__.TestTask1.test_load_comments)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 30, in test_load_comments\n",
      "  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n",
      "    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/comments_csv.gz.\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_load_posts (__main__.TestTask1.test_load_posts)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 16, in test_load_posts\n",
      "  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n",
      "    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/posts_csv.gz.\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_load_users (__main__.TestTask1.test_load_users)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 41, in test_load_users\n",
      "  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n",
      "    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/users_csv.gz.\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_save_dfs (__main__.TestTask1.test_save_dfs)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 60, in test_save_dfs\n",
      "  File \"C:\\Users\\vegar\\AppData\\Local\\Temp\\ipykernel_12540\\3795138152.py\", line 11, in load_csv\n",
      "    df = spark.read.csv(path=source_file, schema=schema, sep='\\t', header=True)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 740, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vegar\\OneDrive - NTNU\\Documents\\Universitetet\\12. Semester\\Big Data Arkitektur\\BigData\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/FileStore/tables/users_csv.gz.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 6.704s\n",
      "\n",
      "FAILED (errors=5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=5 failures=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask1(unittest.TestCase):\n",
    "   \n",
    "    # test 1\n",
    "    def test_load_badges(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/badges_csv.gz\", schema=badges_schema)\n",
    "        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 2\n",
    "    def test_load_posts(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/posts_csv.gz\", schema=posts_schema)\n",
    "        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower,\n",
    "                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n",
    "                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n",
    "                                    'CloseDate']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 3\n",
    "    def test_load_comments(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/comments_csv.gz\", schema=comments_schema)\n",
    "        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 4\n",
    "    def test_load_users(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/users_csv.gz\", schema=users_schema)\n",
    "        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower,\n",
    "                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n",
    "                                    'Views', 'UpVotes', 'DownVotes']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    # test 5\n",
    "    def test_save_dfs(self):\n",
    "        dfs = [(\"/FileStore/tables/users_csv.gz\", users_schema, \"users\"),\n",
    "               (\"/FileStore/tables/badges_csv.gz\", badges_schema, \"badges\"),\n",
    "               (\"/FileStore/tables/comments_csv.gz\", comments_schema, \"comments\"),\n",
    "               (\"/FileStore/tables/posts_csv.gz\", posts_schema, \"posts\")\n",
    "               ]\n",
    "\n",
    "        for i in dfs:\n",
    "            df = load_csv(source_file=i[0], schema=i[1])\n",
    "            save_df(df, i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f99b257-8618-4796-aeb0-d9446863c259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n",
    "\n",
    "Write your descriptions in the next cell.\n",
    "\n",
    "\n",
    "Spark Application: I think a Spark application is a distibuted computing program, which is written in order to perform complex processing of data tasks. It has to run on a cluster managed by the Spark framework. This enables efficient processing of large datasets on multiple nodes. \n",
    "Some core features are in-memory data storage and optimized execution plans, in order to achive high performance for both real-time data and batch processing tasks. \n",
    "\n",
    "\n",
    "SparkSession: SparkSession is a unified entry point for Spark applications. It replaced earlier entry points, including SQLContext, HiveContext and SparkContext. SparkSession gives a more streamlined way to interact with Spark functionalities. This inncludes SQL queries, streaming data, Dataset APIs and DataFrame. \n",
    "By encapsulating multiple contexts into a single object, SparkSession can make it easier for a developer to write and mandage Spark code.Through this single interface, a user can perform data processing tasks, including reads and writes of data to execute SQL queries and performing analytics. \n",
    "\n",
    "\n",
    "Transformation: Transformation is one of the core consepts in Spark. It defines operations on RDDs, Resilient Distributed Datasets to create new Data. Transformations include operations like filter, map, join, groupBy, allowing the manipulation of data across the Spark cluster. \n",
    "Transformations are lazily evaluated. This means they won't be executed immideatily, but ratherbuilds up a lineage of operations to be executed. By using this approach, Spark can optimize the overall pipeline for processing data. Which imporves performance and resource utilization. As they produce new datasets, they are fundamental for Spark's ability to handle big data processing tasks quick and efficent.\n",
    "\n",
    "\n",
    "\n",
    "Action: Actions in Spark is what trigger the execution of the transformations applied to RDDs. Opposed to transformations, which as we know are lazily evaluted, actions force an immediate computation of the results. Some examples of actions includes count, collect, save and take. \n",
    "When an action is aclled Spark evaluates the entire chain of transformation that have been applied to that point. This returns  a result to the Spark driver or writes data to storage. Actions are important to obtain results from Spark applications, whether it's for data output, triggering computations or aggregation results. \n",
    "\n",
    "\n",
    "\n",
    "Lazy Evalution: Lazy Evalution is a key performance optimization feature in Spark. It delays the execution of transformations until an action is called. Spark builds an exection plan, a DAG (Directed Acyclic Graph, of transformations), as opposed to exectue each transformation as soon as it is defined. By doing this, Spark can optimize th eoverall workflow for data processing. It can rearrange operations for efficency, or by minimising the amount of data the cluster collects. Therefore, Lazy evaluations can reduce unnecessary computations and improves the efficienct of Spark applications, by executing transformations in an optimized order, when required by an action. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ec9fda-7848-4f01-8a36-3b82b78be007",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Your descriptions..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2071668267617896,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Task1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
