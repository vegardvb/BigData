{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c461633a",
   "metadata": {},
   "source": [
    "### Enter full names of group members:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4648c5",
   "metadata": {},
   "source": [
    "##### Name A: Vegard Vaeng Bernhardsen\n",
    "##### Name B: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d55dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sympy\n",
    "from pathlib import Path  # for paths of files\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# ANSI escape codes for colors\n",
    "class colors:\n",
    "    red = '\\033[91m'\n",
    "    green = '\\033[92m'\n",
    "    blue = '\\033[94m'\n",
    "    end = '\\033[0m'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4a780",
   "metadata": {},
   "source": [
    "### 1. DGIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287695e",
   "metadata": {},
   "source": [
    "#### 1.1. DGIM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af55744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default DGIM parameters\n",
    "\n",
    "stream_path = 'data/my_stream.txt'\n",
    "\n",
    "# The window size\n",
    "N = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f339cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated list of timestamps buckets from DGIM algorithm: \n",
      " [[99], [91, 96], [83, 89], [63, 75], [44], [6], [321, 446], [188], []]\n",
      "The end timestamp: 99\n"
     ]
    }
   ],
   "source": [
    "def initialize_buckets():\n",
    "    \"\"\"Initialize an empty list to store buckets.\"\"\"\n",
    "    return []\n",
    "\n",
    "def update_timestamp(timestamp, N):\n",
    "    \"\"\"Update the timestamp to simulate a circular buffer.\"\"\"\n",
    "    return (timestamp + 1) % N\n",
    "\n",
    "def remove_old_buckets(buckets, timestamp, N):\n",
    "    \"\"\"Remove buckets that are outside the sliding window.\"\"\"\n",
    "    return [bucket for bucket in buckets if (timestamp - bucket[1]) % N < N - 1]\n",
    "\n",
    "def merge_buckets(buckets):\n",
    "    \"\"\"Merge consecutive buckets of the same size.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(buckets) - 2:\n",
    "        if buckets[i][0] == buckets[i + 1][0] == buckets[i + 2][0]:\n",
    "            new_size = buckets[i][0] * 2\n",
    "            new_timestamp = buckets[i + 1][1]\n",
    "            buckets[i + 1] = (new_size, new_timestamp)\n",
    "            del buckets[i]\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "def dgim_algorithm(stream_path, N):\n",
    "    \"\"\"Implementation of the DGIM algorithm.\"\"\"\n",
    "    buckets = initialize_buckets()\n",
    "    timestamp = 0\n",
    "\n",
    "    with open(stream_path) as f:\n",
    "        while True:\n",
    "            bit = f.read(1)\n",
    "            if not bit:\n",
    "                break\n",
    "\n",
    "            timestamp = update_timestamp(timestamp, N)\n",
    "\n",
    "            buckets = remove_old_buckets(buckets, timestamp, N)\n",
    "\n",
    "            if bit == '1':\n",
    "                buckets.append((1, timestamp))\n",
    "                merge_buckets(buckets)\n",
    "\n",
    "    # Prepare output\n",
    "    bucket_list = [[] for _ in range(math.ceil(math.log2(N)))]\n",
    "    end_time_stamp = None\n",
    "\n",
    "    for size, ts in buckets:\n",
    "        index = int(math.log2(size))\n",
    "        if index < len(bucket_list):\n",
    "            bucket_list[index].append(ts)\n",
    "            if size == 1:\n",
    "                end_time_stamp = ts\n",
    "\n",
    "    for blist in bucket_list:\n",
    "        blist.sort()\n",
    "\n",
    "    return bucket_list, end_time_stamp\n",
    "\n",
    "\n",
    "bucket, end_time_stamp = dgim_algorithm(stream_path, N)\n",
    "print(f\"The updated list of timestamps buckets from DGIM algorithm: \\n {bucket}\")\n",
    "print(f\"The end timestamp: {end_time_stamp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc1d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = dgim_algorithm(stream_path, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6966be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated list of timestamps buckets from DGIM algorithm: \n",
      " [[99], [91, 96], [83, 89], [63, 75], [44], [6], [321, 446], [188], []]\n",
      "The end timestamp: 99\n"
     ]
    }
   ],
   "source": [
    "print(f\"The updated list of timestamps buckets from DGIM algorithm: \\n {bucket[0]}\")\n",
    "print(f\"The end timestamp: {bucket[1]}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c273257",
   "metadata": {},
   "source": [
    "#### 1.2. Query the Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb0343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_count(stream_path, k):\n",
    "    stream_list = []\n",
    "    with open(stream_path, 'r') as file:\n",
    "        for line in file:\n",
    "            stream_list.extend(list(map(int, line.strip())))\n",
    "\n",
    "    # Convert the list into a numpy array\n",
    "    stream_array = np.array(stream_list)\n",
    "    \n",
    "    return int(np.sum(stream_array[-k:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7f130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effective_count(bucket_list, end_time_stamp, N, k):\n",
    "    \"\"\"Calculate the estimated number of 1s in the last k bits based on DGIM buckets.\"\"\"\n",
    "    one_count = 0\n",
    "    last_bucket = 0\n",
    "\n",
    "    # Calculate contributions of buckets within the window\n",
    "    for size, timestamps in enumerate(bucket_list):\n",
    "        bucket_size = 2 ** size\n",
    "        for timestamp in timestamps:\n",
    "            if (end_time_stamp - timestamp) % N < k:\n",
    "                one_count += bucket_size\n",
    "                last_bucket = size\n",
    "\n",
    "    # Adjust by subtracting half the size of the last bucket to reduce overcount\n",
    "    if last_bucket:\n",
    "        one_count -= 2**(last_bucket) / 2\n",
    "\n",
    "    return math.ceil(one_count)\n",
    "\n",
    "def dgim_query(bucket, N, k):\n",
    "    \"\"\"Query the DGIM algorithm for the count of 1s in the last k bits.\"\"\"\n",
    "    bucket_list, end_time_stamp = bucket\n",
    "    return get_effective_count(bucket_list, end_time_stamp, N, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387e5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of queries\n",
    "K = [10, 50, 100, 300, 500] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7702bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "The total 1s in the last 10 bits by DGIM: 4\n",
      "The true count of 1s in the last 10 bits: 5\n",
      "The DGIM error for predicted 1s in the last 10 bits:     20.0 %\n",
      "---------------------------------------------------------------\n",
      "The total 1s in the last 50 bits by DGIM: 25\n",
      "The true count of 1s in the last 50 bits: 26\n",
      "The DGIM error for predicted 1s in the last 50 bits:     3.85 %\n",
      "---------------------------------------------------------------\n",
      "The total 1s in the last 100 bits by DGIM: 61\n",
      "The true count of 1s in the last 100 bits: 51\n",
      "The DGIM error for predicted 1s in the last 100 bits:     19.61 %\n",
      "---------------------------------------------------------------\n",
      "The total 1s in the last 300 bits by DGIM: 173\n",
      "The true count of 1s in the last 300 bits: 150\n",
      "The DGIM error for predicted 1s in the last 300 bits:     15.33 %\n",
      "---------------------------------------------------------------\n",
      "The total 1s in the last 500 bits by DGIM: 269\n",
      "The true count of 1s in the last 500 bits: 241\n",
      "The DGIM error for predicted 1s in the last 500 bits:     11.62 %\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------------------------------------\")\n",
    "for k in K:\n",
    "    dgim_count = dgim_query(bucket, 500, k)\n",
    "    true_count = actual_count(stream_path, k)\n",
    "    \n",
    "    print(f\"The total 1s in the last {k} bits by DGIM: {dgim_count}\")\n",
    "    print(f\"The true count of 1s in the last {k} bits: {true_count}\")\n",
    "    print(f\"The DGIM error for predicted 1s in the last {k} bits: \\\n",
    "    {round(abs(100*(dgim_count-true_count))/true_count,2)} %\")\n",
    "    print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaaceac",
   "metadata": {},
   "source": [
    "### 2. Bloom filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92883c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Username data for the creation of bloom filters - B\n",
    "data_file = (Path(\"data/bloom_username\").with_suffix('.csv'))\n",
    "\n",
    "# Test data to check the functionality and false positive rate\n",
    "test1_file = (Path(\"data/test1_username\").with_suffix('.csv'))\n",
    "test2_file = (Path(\"data/test2_username\").with_suffix('.csv'))\n",
    "\n",
    "# Default bloom filter parameters\n",
    "bloom_size = 1500000 # parameter N\n",
    "h = 3 # number of hash functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5e5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of bloom filter with zeros\n",
    "B = np.zeros(bloom_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c033746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73d660",
   "metadata": {},
   "source": [
    "#### 2.1. Create Bloom filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b69edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash(h, N):\n",
    "    hash_list = []\n",
    "    used_primes = set()\n",
    "    \n",
    "    for _ in range(h):\n",
    "        random_prime = generate_unique_prime(used_primes)\n",
    "        hash_function = create_hash_function(random_prime, N)\n",
    "        hash_list.append(hash_function)\n",
    "    \n",
    "    return hash_list\n",
    "\n",
    "def generate_unique_prime(used_primes):\n",
    "    while True:\n",
    "        random_prime = sympy.randprime(1, 10000000)\n",
    "        if random_prime not in used_primes:\n",
    "            used_primes.add(random_prime)\n",
    "            return random_prime\n",
    "\n",
    "def create_hash_function(prime_number, N):\n",
    "    def hash_function(s):\n",
    "        if prime_number is not None:\n",
    "            return sum((ord(c) * pow(prime_number, i, N) for i, c in enumerate(s))) % N\n",
    "        else:\n",
    "            raise ValueError(\"Prime number cannot be None\")\n",
    "    return hash_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75aeecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = generate_hash(h, bloom_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2d4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bloom_filter(B, hashes, data):\n",
    "    with data.open(encoding='utf-8') as f:  # Specify encoding as 'utf-8'\n",
    "        for name in f:\n",
    "            name = name.strip()  # Remove leading/trailing whitespaces\n",
    "            for hash_func in hashes:\n",
    "                index = hash_func(name)\n",
    "                B[index] = 1  # Set the corresponding bit in the Bloom filter array to 1\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe79b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "bloom_array = create_bloom_filter(B, hashes, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ce957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff48616",
   "metadata": {},
   "source": [
    "#### 2.2. Verify usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "530485d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_verify_username(bloom_array, hashes, new_user):\n",
    "    for hash_func in hashes:\n",
    "        index = hash_func(new_user)\n",
    "        if bloom_array[index] == 0:\n",
    "            return 0  # Username is available\n",
    "    return 1  # Username is likely taken\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6edf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to test different usernames here\n",
    "\n",
    "new_username = \"KazeemTDT4305\"\n",
    "\n",
    "# new_username = \"ShambaTDT4305\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22690d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_code = single_verify_username(bloom_array, hashes, new_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7730361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mUsername KazeemTDT4305 has been taken. Try again!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if user_code == 1:\n",
    "    print(colors.red + f\"Username {new_username} has been taken. Try again!\" + colors.end)\n",
    "elif user_code == 0:\n",
    "    print(colors.green + f\"Username {new_username} is available. Congrats!\" + colors.end)\n",
    "else:\n",
    "    print(colors.blue + f\"Wrong pass code. Please reverify!\" + colors.end)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "080d7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_verify_username(bloom_array, hashes, data):\n",
    "    # Initialize counts\n",
    "    total_name = 0\n",
    "    taken_name = 0\n",
    "    \n",
    "    with data.open(encoding='utf-8') as f:\n",
    "        for name in f:\n",
    "            name = name.strip()  # Remove leading/trailing whitespaces\n",
    "            total_name += 1\n",
    "            taken_name += single_verify_username(bloom_array, hashes, name)\n",
    "            \n",
    "    return round(taken_name/total_name*100,2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4725c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of username seen before from test 1: 100.0%\n",
      "----------------------------------------------------------\n",
      "Percentage of username seen before from test 2: 23.67%\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------------------------------------\")\n",
    "user_total = group_verify_username(bloom_array, hashes, test1_file)\n",
    "print(f\"Percentage of username seen before from test 1: {user_total}%\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "user_total = group_verify_username(bloom_array, hashes, test2_file)\n",
    "print(f\"Percentage of username seen before from test 2: {user_total}%\")\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488c00b",
   "metadata": {},
   "source": [
    "### 3. Flajolet-Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae74f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flajolet_martin(input_stream):\n",
    "    R = 0  # Initialize maximum rightmost zero bit position to 0\n",
    "\n",
    "    # Define hash function h(x) = 6x + 1 mod 5\n",
    "    def hash_function(x):\n",
    "        return (6 * x + 1) % 5\n",
    "\n",
    "    # Iterate over the input stream and update maximum rightmost zero bit position\n",
    "    for element in input_stream:\n",
    "        hash_value = hash_function(element)\n",
    "        binary_rep = bin(hash_value)[2:]  # Convert hash value to binary representation\n",
    "        rightmost_zero_bit = len(binary_rep) - binary_rep.rfind('0') - 1\n",
    "        if rightmost_zero_bit > R:\n",
    "            R = rightmost_zero_bit\n",
    "\n",
    "    # Estimate the number of distinct elements\n",
    "    distinct_estimate = 2 ** R\n",
    "\n",
    "    return distinct_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7a283b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Distinct elements (estimated) in input stream 1: 4\n",
      "-----------------------------------------------------\n",
      "Distinct elements (estimated) in input stream 2: 4\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input stream\n",
    "input_stream1 = [1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1]\n",
    "input_stream2 = [1, 3, 2, 1, 2, 3, 4, 3, 1, 2, 3, 1]\n",
    "\n",
    "# Run the Flajolet-Martin algorithm\n",
    "distinct_estimate1 = flajolet_martin(input_stream1)\n",
    "distinct_estimate2 = flajolet_martin(input_stream2)\n",
    "\n",
    "# Print the estimated number of distinct elements\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(f\"Distinct elements (estimated) in input stream 1:\", distinct_estimate1)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(f\"Distinct elements (estimated) in input stream 2:\", distinct_estimate2)\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3051ee5",
   "metadata": {},
   "source": [
    "### 4. Adword "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b08ba",
   "metadata": {},
   "source": [
    "#### 4.1. Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a58d6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User queries\n",
    "queries = [\"big data\", \"big data\", \"big data\",\"bloom filters\", \"bloom filters\", \"bloom filters\",\n",
    "           \"flajolet martin\", \"flajolet martin\", \"flajolet martin\", \"dgim algorithm\", \"dgim algorithm\", \"dgim algorithm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66ee11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company A B C and D keywords and budget $$$\n",
    "global_companies = {\n",
    "        'A': [\"big data\", \"bloom filters\", 3],\n",
    "        'B': [\"flajolet martin\", 3],\n",
    "        'C': [\"flajolet martin\", \"dgim algorithm\", 3],\n",
    "        'D': [\"big data\", 3],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd6eb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_algorithm(companies, queries):\n",
    "    revenue = 0\n",
    "    \n",
    "    for query in queries:\n",
    "        potential_company = []\n",
    "\n",
    "        for company, i in companies.items():\n",
    "            keywords, budget = i[:-1], i[-1]\n",
    "            \n",
    "            if query in keywords and budget > 0:\n",
    "                potential_company.append(company)\n",
    "            \n",
    "        if potential_company:\n",
    "            chosen_bidder = random.choice(potential_company)\n",
    "            revenue += 1  # Increment revenue by 1 for each query-advertiser match\n",
    "\n",
    "            companies[chosen_bidder][-1] -= 1\n",
    "    \n",
    "    return revenue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c9378f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trials using Greedy Algorithm...\n",
      "------------------------------------------------\n",
      "Trial 1 - Revenue generated: 11\n",
      "Trial 2 - Revenue generated: 10\n",
      "Trial 3 - Revenue generated: 9\n",
      "Trial 4 - Revenue generated: 11\n",
      "Trial 5 - Revenue generated: 8\n",
      "Trial 6 - Revenue generated: 10\n",
      "Trial 7 - Revenue generated: 10\n",
      "Trial 8 - Revenue generated: 10\n",
      "Trial 9 - Revenue generated: 8\n",
      "Trial 10 - Revenue generated: 10\n",
      "------------------------------------------------\n",
      "Average revenue generated for all trials:  9.7\n"
     ]
    }
   ],
   "source": [
    "total_revenue = 0\n",
    "total_trials = 10\n",
    "print(\"Starting trials using Greedy Algorithm...\")\n",
    "print(\"------------------------------------------------\")\n",
    "for i in range(total_trials):\n",
    "    companies = copy.deepcopy(global_companies)\n",
    "    revenue = greedy_algorithm(companies, queries)\n",
    "    total_revenue = total_revenue + revenue\n",
    "    print(f\"Trial {i+1} - Revenue generated: {revenue}\")\n",
    "print(\"------------------------------------------------\")   \n",
    "print(\"Average revenue generated for all trials: \",total_revenue/total_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fda97",
   "metadata": {},
   "source": [
    "#### 4.2. Balance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9af1b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_algorithm(companies, queries):\n",
    "    revenue = 0\n",
    "    \n",
    "    for query in queries:\n",
    "        potential_advertisers = []\n",
    "\n",
    "        # Collect all advertisers with matching keywords and positive budget\n",
    "        for company, data in companies.items():\n",
    "            keywords, budget = data[:-1], data[-1]\n",
    "            if query in keywords and budget > 0:\n",
    "                potential_advertisers.append((company, budget))\n",
    "\n",
    "        # If there are potential advertisers, proceed with selection\n",
    "        if potential_advertisers:\n",
    "            # Sort advertisers by budget, descending, and find the highest budget\n",
    "            potential_advertisers.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_budget = potential_advertisers[0][1]\n",
    "\n",
    "            # Filter for advertisers with the highest budget\n",
    "            highest_budget_advertisers = [adv[0] for adv in potential_advertisers if adv[1] == highest_budget]\n",
    "\n",
    "            # Randomly select one advertiser from those with the highest budget\n",
    "            chosen_bidder = random.choice(highest_budget_advertisers)\n",
    "            revenue += 1  # Increment revenue for each match\n",
    "\n",
    "            # Decrement the budget of the chosen advertiser\n",
    "            companies[chosen_bidder][-1] -= 1\n",
    "    \n",
    "    return revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b975413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trials using Balance Algorithm...\n",
      "-------------------------------------------\n",
      "Trial 1 - Revenue generated: 9\n",
      "Trial 2 - Revenue generated: 9\n",
      "Trial 3 - Revenue generated: 8\n",
      "Trial 4 - Revenue generated: 9\n",
      "Trial 5 - Revenue generated: 8\n",
      "Trial 6 - Revenue generated: 10\n",
      "Trial 7 - Revenue generated: 9\n",
      "Trial 8 - Revenue generated: 10\n",
      "Trial 9 - Revenue generated: 10\n",
      "Trial 10 - Revenue generated: 9\n",
      "-------------------------------------------\n",
      "Average revenue generated for all trials:  9.1\n"
     ]
    }
   ],
   "source": [
    "total_revenue = 0\n",
    "total_trials = 10\n",
    "print(\"Starting trials using Balance Algorithm...\")\n",
    "print(\"-------------------------------------------\")\n",
    "for i in range(total_trials):\n",
    "    companies = copy.deepcopy(global_companies)\n",
    "    revenue = balance_algorithm(companies, queries)\n",
    "    total_revenue = total_revenue + revenue\n",
    "    print(f\"Trial {i+1} - Revenue generated: {revenue}\")\n",
    "print(\"-------------------------------------------\")   \n",
    "print(\"Average revenue generated for all trials: \",total_revenue/total_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2ef9e",
   "metadata": {},
   "source": [
    "### 5. Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86174f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings matrix (each row corresponds to a movie, and each column corresponds to a user)\n",
    "ratings_matrix = np.array([\n",
    "    [1, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 0],\n",
    "    [0, 0, 5, 4, 0, 0, 4, 0, 0, 2, 1, 3],\n",
    "    [2, 4, 0, 1, 2, 0, 3, 0, 4, 3, 5, 0],\n",
    "    [0, 2, 4, 0, 5, 0, 0, 4, 0, 0, 2, 0],\n",
    "    [0, 0, 4, 3, 4, 2, 0, 0, 0, 0, 2, 5],\n",
    "    [1, 0, 3, 0, 3, 0, 0, 2, 0, 0, 4, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92e8e0",
   "metadata": {},
   "source": [
    "#### 5.1. User-User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0749438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ratings(rate_m, user_index):\n",
    "    \"\"\"Get ratings for a specific user.\"\"\"\n",
    "    return rate_m[:, user_index].reshape(1, -1)\n",
    "\n",
    "def compute_similarity(r_x, r_y):\n",
    "    \"\"\"Compute cosine similarity between two rating vectors.\"\"\"\n",
    "    return cosine_similarity(r_x, r_y)[0][0]\n",
    "\n",
    "def user_cf(rate_m, tup_mu, neigh):\n",
    "    \"\"\"User-User Collaborative Filtering using cosine similarity.\"\"\"\n",
    "    # Extract movie index and user index\n",
    "    movie_index, user_index = tup_mu[0] - 1, tup_mu[1] - 1\n",
    "    \n",
    "    # Get ratings for the target user\n",
    "    r_x = get_user_ratings(rate_m, user_index)\n",
    "\n",
    "    # Compute similarity between the target user and all other users\n",
    "    most_similar = []\n",
    "    for i in range(len(rate_m[0])):\n",
    "        if i == user_index:\n",
    "            continue\n",
    "        \n",
    "        r_y = get_user_ratings(rate_m, i)\n",
    "        sim = compute_similarity(r_x, r_y)\n",
    "        most_similar.append((i, sim))\n",
    "    \n",
    "    # Sort users by similarity and limit to the specified neighborhood\n",
    "    most_similar = sorted(most_similar, key=lambda x: x[1], reverse=True)[:neigh]\n",
    "\n",
    "    # Compute weighted sum of ratings and total similarity\n",
    "    weighted_ratings, total_sim = 0, 0\n",
    "    for user, sim in most_similar:\n",
    "        movie_rating = rate_m[movie_index, user]\n",
    "        weighted_ratings += sim * movie_rating\n",
    "        total_sim += sim\n",
    "    \n",
    "    # Handle division by zero\n",
    "    if total_sim == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Compute the predicted rating\n",
    "    prediction = round(weighted_ratings / total_sim, 2)\n",
    "    \n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c153de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tuple of movie rating by users to be predicted e.g (1, 5) refers to the rating of movie 1 by user 5\n",
    "list_mu_query = [(1, 5), (3, 3)]\n",
    "\n",
    "# Neighbor selection (|N|)\n",
    "neigh = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22f8e8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "The predicted rating of movie 1 by user 5: 1.42 (User-User CF)\n",
      "-----------------------------------------------------------------\n",
      "The predicted rating of movie 3 by user 3: 1.49 (User-User CF)\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------\")   \n",
    "for mu_query in list_mu_query:\n",
    "    predicted_rating = user_cf(ratings_matrix, mu_query, neigh)\n",
    "    print(f\"The predicted rating of movie {mu_query[0]} by user {mu_query[1]}: {predicted_rating} (User-User CF)\")\n",
    "    print(\"-----------------------------------------------------------------\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217e4ed",
   "metadata": {},
   "source": [
    "#### 5.2. Item-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c03be5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_ratings(rate_m, movie_index):\n",
    "    \"\"\"Get ratings for a specific movie.\"\"\"\n",
    "    return rate_m[movie_index, :].reshape(1, -1)\n",
    "\n",
    "def compute_similarity(r_x, r_y):\n",
    "    \"\"\"Compute cosine similarity between two rating vectors.\"\"\"\n",
    "    return cosine_similarity(r_x, r_y)[0][0]\n",
    "\n",
    "def item_cf(rate_m, tup_mu, neigh):\n",
    "    \"\"\"Item-Item Collaborative Filtering using cosine similarity.\"\"\"\n",
    "    # Extract movie index and user index\n",
    "    movie_index, user_index = tup_mu[0] - 1, tup_mu[1] - 1\n",
    "    \n",
    "    # Get ratings for the target movie\n",
    "    r_x = get_movie_ratings(rate_m, movie_index)\n",
    "\n",
    "    # Compute similarity between the target movie and all other movies\n",
    "    most_similar = []\n",
    "    for i in range(len(rate_m)):\n",
    "        if i == movie_index:\n",
    "            continue\n",
    "        \n",
    "        r_y = get_movie_ratings(rate_m, i)\n",
    "        sim = compute_similarity(r_x, r_y)\n",
    "        most_similar.append((i, sim))\n",
    "    \n",
    "    # Sort movies by similarity and limit to the specified neighborhood\n",
    "    most_similar = sorted(most_similar, key=lambda x: x[1], reverse=True)[:neigh]\n",
    "\n",
    "    # Compute weighted sum of ratings and total similarity\n",
    "    weighted_ratings, total_sim = 0, 0\n",
    "    for movie, sim in most_similar:\n",
    "        movie_rating = rate_m[movie, user_index]\n",
    "        weighted_ratings += sim * movie_rating\n",
    "        total_sim += sim\n",
    "    \n",
    "    # Handle division by zero\n",
    "    if total_sim == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Compute the predicted rating\n",
    "    prediction = round(weighted_ratings / total_sim, 2)\n",
    "    \n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4b5ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "The predicted rating of movie 1 by user 5: 2.48 (Item-Item CF)\n",
      "-----------------------------------------------------------------\n",
      "The predicted rating of movie 3 by user 3: 3.0 (Item-Item CF)\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------\")   \n",
    "for mu_query in list_mu_query:\n",
    "    predicted_rating = item_cf(ratings_matrix, mu_query, neigh)\n",
    "    print(f\"The predicted rating of movie {mu_query[0]} by user {mu_query[1]}: {predicted_rating} (Item-Item CF)\")\n",
    "    print(\"-----------------------------------------------------------------\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892ce96",
   "metadata": {},
   "source": [
    "### Provide concise answers to all 5 cases in the Project 3 description below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc34aad",
   "metadata": {},
   "source": [
    "#### Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669b54b",
   "metadata": {},
   "source": [
    "It's true the number of buckets used in DGIM scales scales logarthmically with the size of the sliding window N, the actual space complexity is determined by the amount of memory each bucket consumes. \n",
    "Each bucket stores O(log n) bits for the timestamp. So there are O(log n) buckets wich all store O(log n) bits. Thus the space complexity of the DGIM algorithm is O(log^2 n).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b85a6",
   "metadata": {},
   "source": [
    "#### Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8340d9f",
   "metadata": {},
   "source": [
    "If the bloom filter outputs that \"Kazeem\" is taken then their is a certain possibility that this is a false positive. This is because with bloom filters, although with create memory usage, it could give out false positives. I would additionally tell the admin that this is highly correlated to the use of hash functions. It is possible that somehow all the hash functions consider a username to be taken, due to imperfected hashes. \n",
    "\n",
    "In this case the bloom filter outputs that a username has NOT been taken. A bloom filter guarantees no false negatives, hence it is impossible that the username \"KazeemTDT4305\" has been taken. For the bloom filter to return that a username has NOT been taken, only one of the indexes given by one hash function in the bloom filter must be 0. This will not happen if the username was taken. \n",
    "\n",
    "The reason that Google finds that particular email to be taken could be because of false positives when using bloom fitlers as discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16cad2",
   "metadata": {},
   "source": [
    "#### Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9b9fa",
   "metadata": {},
   "source": [
    "To improve precision with the Flajolet-Martin algorithm, we employ multiple hash functions grouped into sets. Instead of relying on just one hash function, using many allows us to gather diverse data from the input stream, reducing the chance of errors. Each group of hash functions works independently, providing unique insights into the data. Within each group, we choose the median value to minimize the impact of outliers. Finally, we calculate the average of all median values obtained from different groups, resulting in a more accurate estimate of the distinct count. By following this method, we enhance the accuracy of the Flajolet-Martin algorithm's estimates, making it more reliable for estimating the number of unique elements in a data stream.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9e628",
   "metadata": {},
   "source": [
    "#### Case 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78ac31",
   "metadata": {},
   "source": [
    "# Performance Analysis of Greedy and Balance Algorithms\n",
    "\n",
    "**Maximum Possible Revenue**\n",
    "\n",
    "For maximum revenue:\n",
    "- **Company D** handles all queries for 'big data', earning **3 dollars**. Company D's budget is then depleted.\n",
    "- **Company A** addresses all queries for 'bloom filters', adding **3 dollars** for a total of **6 dollars**. Company A's budget is used up.\n",
    "- **Company B** covers all queries for 'flajolet martin', increasing the total to **9 dollars**. Company B's budget is spent.\n",
    "- **Company C** manages all queries for 'dgim algorithm', reaching a total revenue of **12 dollars**. Company C's budget is exhausted.\n",
    "\n",
    "This optimal distribution results in a total revenue of **12 dollars**.\n",
    "\n",
    "**Minimum Possible Revenue**\n",
    "\n",
    "For the minimum revenue:\n",
    "- **Company A** takes all queries for 'big data', earning **3 dollars**. Company A's budget is depleted.\n",
    "- **Company C** takes all queries for 'flajolet martin', making the total revenue **6 dollars**. Company C's budget is spent.\n",
    "\n",
    "The remaining queries ('bloom filters' and 'dgim algorithm') do not match the interests of the remaining companies with available budgets, resulting in a minimum revenue of **6 dollars**.\n",
    "\n",
    "**Competitive Ratio for the Greedy Algorithm**\n",
    "\n",
    "The Greedy algorithm could randomly pick bidders that lead to this minimum revenue scenario:\n",
    "- By potentially picking advertisers in a manner that matches the minimum revenue scenario, the Greedy algorithm's worst performance would be a revenue of **6 dollars**.\n",
    "\n",
    "The competitive ratio thus becomes:\n",
    "\n",
    "Competitive ratio (Greedy) = 1/2\n",
    "\n",
    "**Competitive Ratio for the Balance Algorithm**\n",
    "\n",
    "In the worst case for the Balance algorithm:\n",
    "- **Company A** takes 2 queries for 'big data' and 1 for 'bloom filter', earning **3 dollars**.\n",
    "- **Company B** takes 1 query for 'flajolet martin', contributing **1 dollar**.\n",
    "- **Company C** addresses 2 queries for 'flajolet martin' and 1 for 'dgim algorithm', adding **3 dollars**.\n",
    "- **Company D** handles 1 query for 'big data', adding **1 dollar**.\n",
    "\n",
    "This yields a total revenue of **8 dollars**.\n",
    "\n",
    "The competitive ratio thus becomes:\n",
    "\n",
    "Competitive ratio (Balance) = 2/3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c341065",
   "metadata": {},
   "source": [
    "#### Case 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aa9df",
   "metadata": {},
   "source": [
    "Upon analyzing the ratings given by user 5, we observe that their average rating across all movies is **3.5**. Comparing this to the predictions made by User-User and Item-Item Collaborative Filtering for movie 1, the User-User CF predicts a rating of **1.42**, while the Item-Item CF predicts a rating of **2.42**. Given that the Item-Item CF's prediction is closer to user 5's average rating, it suggests that this method might provide a more accurate estimation in this case.\n",
    "\n",
    "Furthermore, evaluating the overall quality of movie 1 based on its average rating (which is **4.5**), we can deduce that movie 1 is generally received well and not a poorly rated movie. This context reinforces the idea that the higher prediction (**2.42**) from Item-Item CF aligns better with the general perception of the movie. Therefore, for this scenario, Item-Item CF appears to offer a more reliable and realistic prediction, capturing both the trends in user 5’s ratings and the movie’s general appeal more effectively.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
